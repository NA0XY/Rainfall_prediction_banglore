{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f52e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas earthengine-api requests rasterio rasterstats scikit-learn pandas matplotlib statsmodels tensorflow shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import ee\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "ee.Authenticate()\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "print(\"Earth Engine Initialized Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea42c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_path = '5th cite.gpkg'\n",
    "\n",
    "print(\"Defining Area of Interest from ward boundaries...\")\n",
    "\n",
    "wards_gdf = gpd.read_file(gpkg_path, layer='wards_2022')\n",
    "\n",
    "wards_gdf = wards_gdf.to_crs(\"EPSG:4326\")\n",
    "print(f\"  - Step 1: Reprojected data to WGS84 (EPSG:4326).\")\n",
    "\n",
    "bounds = wards_gdf.total_bounds\n",
    "print(f\"  - Step 2: Calculated total bounds: {bounds}\")\n",
    "\n",
    "aoi = ee.Geometry.Rectangle(*bounds)\n",
    "\n",
    "print(\"\\n AOI defined successfully using a robust bounding box.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa96f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing high-resolution data for download...\")\n",
    "\n",
    "dem = ee.Image('USGS/SRTMGL1_003').clip(aoi)\n",
    "slope = ee.Terrain.slope(dem)\n",
    "print(\"  - DEM and Slope prepared.\")\n",
    "\n",
    "lulc_recent = ee.ImageCollection('MODIS/061/MCD12Q1').sort('system:time_start', False).first().select('LC_Type1').clip(aoi)\n",
    "print(\"  - Most recent LULC prepared.\")\n",
    "\n",
    "download_dir = 'Flood_Prediction_Data_Local'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "def download_image(image, filename, scale, directory, aoi_geom):\n",
    "    path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': aoi_geom})\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(path, 'wb') as f: f.write(response.content)\n",
    "        if os.path.exists(path): print(f\"\u2705 Successfully downloaded and verified: {path}\\n\")\n",
    "        else: print(f\"\u274c DOWNLOAD FAILED: {path} was not created.\\n\")\n",
    "    except Exception as e: print(f\"\u274c Failed to download {filename}. Reason: {e}\\n\")\n",
    "\n",
    "download_image(dem, 'dem.zip', 30, download_dir, aoi)\n",
    "download_image(slope, 'slope.zip', 30, download_dir, aoi)\n",
    "download_image(lulc_recent, 'lulc_2023.zip', 500, download_dir, aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad22188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unzipping files...\")\n",
    "zip_files_to_unzip = ['dem.zip', 'slope.zip', 'lulc_2023.zip']\n",
    "tif_files_to_check = ['dem.tif', 'slope.tif', 'lulc_2023.tif']\n",
    "\n",
    "for zip_file in zip_files_to_unzip:\n",
    "    path_to_zip = os.path.join(download_dir, zip_file)\n",
    "    if os.path.exists(path_to_zip):\n",
    "        with zipfile.ZipFile(path_to_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(download_dir)\n",
    "        print(f\"  - Extracted: {zip_file}\")\n",
    "\n",
    "print(\"\\n--- Verifying Extracted Files ---\")\n",
    "all_files_found = True\n",
    "for tif_file in tif_files_to_check:\n",
    "    path_to_tif = os.path.join(download_dir, tif_file)\n",
    "    if os.path.exists(path_to_tif): print(f\"\u2705 Found: {path_to_tif}\")\n",
    "    else:\n",
    "        print(f\"\u274c MISSING: {path_to_tif}\")\n",
    "        all_files_found = False\n",
    "\n",
    "if not all_files_found:\n",
    "    print(\"\\n\u274c One or more .tif files are missing. Please re-run the download cell.\")\n",
    "else:\n",
    "    print(\"\\n\u2705 All necessary static files are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e556cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "class KerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, build_fn=None, epochs=100, batch_size=32, verbose=0, **kwargs):\n",
    "        self.build_fn = build_fn\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.kwargs = kwargs\n",
    "        self.model = None\n",
    "        self.label_encoder = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def build_model(self):\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(self.input_shape_,)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(self.num_classes_, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.input_shape_ = X.shape[1]\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        self.num_classes_ = len(self.classes_)\n",
    "        y_categorical = to_categorical(y_encoded, num_classes=self.num_classes_)\n",
    "\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.build_model()\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.kwargs)\n",
    "\n",
    "        self.model.fit(X, y_categorical,\n",
    "                      epochs=self.epochs,\n",
    "                      batch_size=self.batch_size,\n",
    "                      verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.model.predict(X, verbose=0)\n",
    "        predicted_classes = np.argmax(probabilities, axis=1)\n",
    "        return self.label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X, verbose=0)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking drainage sewer data quality...\")\n",
    "\n",
    "drainage_primary = gpd.read_file(gpkg_path, layer='swd_primary')\n",
    "drainage_secondary = gpd.read_file(gpkg_path, layer='swd_secondary')\n",
    "drainage_tertiary = gpd.read_file(gpkg_path, layer='swd_tertiary')\n",
    "\n",
    "print(f\"Primary drainage: {len(drainage_primary)} segments\")\n",
    "print(f\"Secondary drainage: {len(drainage_secondary)} segments\")\n",
    "print(f\"Tertiary drainage: {len(drainage_tertiary)} segments\")\n",
    "\n",
    "empty_primary = drainage_primary.geometry.is_empty.sum()\n",
    "empty_secondary = drainage_secondary.geometry.is_empty.sum()\n",
    "empty_tertiary = drainage_tertiary.geometry.is_empty.sum()\n",
    "\n",
    "print(f\"Empty geometries - Primary: {empty_primary}, Secondary: {empty_secondary}, Tertiary: {empty_tertiary}\")\n",
    "\n",
    "total_length_primary = drainage_primary.length.sum() if not drainage_primary.empty else 0\n",
    "total_length_secondary = drainage_secondary.length.sum() if not drainage_secondary.empty else 0\n",
    "total_length_tertiary = drainage_tertiary.length.sum() if not drainage_tertiary.empty else 0\n",
    "\n",
    "print(f\"Total lengths - Primary: {total_length_primary:.2f}, Secondary: {total_length_secondary:.2f}, Tertiary: {total_length_tertiary:.2f}\")\n",
    "\n",
    "if total_length_primary < 1000 or total_length_secondary < 5000 or total_length_tertiary < 10000 or empty_primary > 0 or empty_secondary > 0 or empty_tertiary > 0:\n",
    "    print(\"\u26a0\ufe0f Drainage data appears inadequate. Consider extracting from Earth Engine.\")\n",
    "    use_ee_drainage = True\n",
    "else:\n",
    "    print(\"\u2705 Drainage data looks good.\")\n",
    "    use_ee_drainage = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRequesting server-side daily rainfall calculation for each ward...\")\n",
    "\n",
    "wards = wards_gdf.copy()\n",
    "\n",
    "ward_identifier_column = 'Name'\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2024-12-31'\n",
    "daily_collection = (\n",
    "    ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n",
    "    .filterDate(start_date, end_date)\n",
    "    .select('precipitation')\n",
    " )\n",
    "\n",
    "projected_crs = wards.estimate_utm_crs()\n",
    "wards_centroids = wards.to_crs(projected_crs).centroid.to_crs(wards.crs)\n",
    "ee_features = []\n",
    "for idx, geom in zip(wards.index, wards_centroids):\n",
    "    geom_json = gpd.GeoSeries([geom]).__geo_interface__\n",
    "    coords = geom_json['features'][0]['geometry']\n",
    "    ee_feature = ee.Feature(ee.Geometry(coords), {'original_id': int(idx)})\n",
    "    ee_features.append(ee_feature)\n",
    "wards_ee_centroids = ee.FeatureCollection(ee_features)\n",
    "print(\"  - Successfully created server-side FeatureCollection of ward centroids.\")\n",
    "\n",
    "print(\"  - Downloading daily rainfall time series for each ward (this may take several minutes)...\")\n",
    "all_rainfall_rows = []\n",
    "wards_list = wards_ee_centroids.toList(wards_ee_centroids.size())\n",
    "num_wards = wards_ee_centroids.size().getInfo()\n",
    "\n",
    "for i in range(num_wards):\n",
    "    ward_feature = ee.Feature(wards_list.get(i))\n",
    "    ward_info = ward_feature.getInfo()\n",
    "    ward_id = ward_info['properties']['original_id']\n",
    "    try:\n",
    "        region_data = daily_collection.getRegion(\n",
    "            geometry=ward_feature.geometry(),\n",
    "            scale=5566\n",
    "        ).getInfo()\n",
    "    except Exception as e:\n",
    "        print(f\"    \u274c Failed to fetch data for ward {ward_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if len(region_data) <= 1:\n",
    "        print(f\"    \u26a0\ufe0f No rainfall data for ward {ward_id}.\")\n",
    "        continue\n",
    "\n",
    "    headers = region_data[0]\n",
    "    data_rows = region_data[1:]\n",
    "    ward_df = pd.DataFrame(data_rows, columns=headers)\n",
    "    ward_df = ward_df[['time', 'precipitation']].dropna()\n",
    "    if ward_df.empty:\n",
    "        continue\n",
    "    ward_df['date'] = pd.to_datetime(ward_df['time'], unit='ms').dt.date.astype(str)\n",
    "    ward_df['rainfall'] = ward_df['precipitation'].astype(float)\n",
    "    ward_df['original_id'] = ward_id\n",
    "    all_rainfall_rows.append(ward_df[['original_id', 'date', 'rainfall']])\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"    ... processed {i + 1} of {num_wards} wards.\")\n",
    "\n",
    "if not all_rainfall_rows:\n",
    "    raise RuntimeError(\"No rainfall data retrieved. Please verify the AOI and date range.\")\n",
    "\n",
    "rainfall_df_long = pd.concat(all_rainfall_rows, ignore_index=True)\n",
    "rainfall_df_long = rainfall_df_long.drop_duplicates(subset=['original_id', 'date'])\n",
    "rainfall_df = rainfall_df_long.pivot(index='original_id', columns='date', values='rainfall').sort_index(axis=1)\n",
    "rainfall_df = rainfall_df.fillna(0)\n",
    "\n",
    "csv_path = os.path.join(download_dir, 'ward_daily_rainfall.csv')\n",
    "rainfall_df.to_csv(csv_path)\n",
    "print(f\"\\n\u2705 Successfully processed and saved all rainfall data to '{csv_path}'.\")\n",
    "wards = wards.join(rainfall_df)\n",
    "\n",
    "date_columns = [\n",
    "    col for col in wards.columns\n",
    "    if isinstance(col, str) and col[:4].isdigit() and '-' in col\n",
    " ]\n",
    "wards[date_columns] = wards[date_columns].fillna(0)\n",
    "wards['average_rainfall'] = wards[date_columns].mean(axis=1)\n",
    "wards['total_rainfall'] = wards[date_columns].sum(axis=1)\n",
    "wards['max_daily_rainfall'] = wards[date_columns].max(axis=1)\n",
    "wards['rainfall_std'] = wards[date_columns].std(axis=1).fillna(0)\n",
    "\n",
    "print(\"\u2705 Feature extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95651ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtracting static geospatial features from downloaded rasters...\")\n",
    "\n",
    "wards_clean = wards[~wards.geometry.is_empty].copy()\n",
    "\n",
    "dem_path = os.path.join(download_dir, 'dem.tif')\n",
    "slope_path = os.path.join(download_dir, 'slope.tif')\n",
    "lulc_path = os.path.join(download_dir, 'lulc_2023.tif')\n",
    "\n",
    "if os.path.exists(dem_path):\n",
    "    dem_stats = zonal_stats(wards_clean, dem_path, stats=\"mean\", nodata=-9999)\n",
    "    wards.loc[wards_clean.index, 'elevation'] = [stat['mean'] if stat['mean'] is not None else 0 for stat in dem_stats]\n",
    "    print(\"  - Elevation extracted.\")\n",
    "else:\n",
    "    wards['elevation'] = 0\n",
    "    print(\"  - DEM file not found, setting elevation to 0.\")\n",
    "\n",
    "if os.path.exists(slope_path):\n",
    "    slope_stats = zonal_stats(wards_clean, slope_path, stats=\"mean\", nodata=-9999)\n",
    "    wards.loc[wards_clean.index, 'slope'] = [stat['mean'] if stat['mean'] is not None else 0 for stat in slope_stats]\n",
    "    print(\"  - Slope extracted.\")\n",
    "else:\n",
    "    wards['slope'] = 0\n",
    "    print(\"  - Slope file not found, setting slope to 0.\")\n",
    "\n",
    "if os.path.exists(lulc_path):\n",
    "    lulc_stats = zonal_stats(wards_clean, lulc_path, stats=\"majority\", nodata=-9999)\n",
    "    wards.loc[wards_clean.index, 'land_cover'] = [stat['majority'] if stat['majority'] is not None else 0 for stat in lulc_stats]\n",
    "    print(\"  - Land cover extracted.\")\n",
    "else:\n",
    "    wards['land_cover'] = 0\n",
    "    print(\"  - LULC file not found, setting land_cover to 0.\")\n",
    "\n",
    "wards['elevation'] = wards['elevation'].fillna(0)\n",
    "wards['slope'] = wards['slope'].fillna(0)\n",
    "wards['land_cover'] = wards['land_cover'].fillna(0)\n",
    "\n",
    "print(\"\u2705 Static geospatial features extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "if not use_ee_drainage:\n",
    "    print(\"\\nCalculating Drainage Density from GPKG...\")\n",
    "    wards_cleaned = wards[~wards.geometry.is_empty].copy()\n",
    "    drainage_primary = gpd.read_file(gpkg_path, layer='swd_primary').to_crs(wards.crs)\n",
    "    drainage_secondary = gpd.read_file(gpkg_path, layer='swd_secondary').to_crs(wards.crs)\n",
    "    drainage_tertiary = gpd.read_file(gpkg_path, layer='swd_tertiary').to_crs(wards.crs)\n",
    "    all_drainage = pd.concat([drainage_primary, drainage_secondary, drainage_tertiary])\n",
    "\n",
    "    projected_crs = wards_cleaned.estimate_utm_crs()\n",
    "    wards_proj = wards_cleaned.to_crs(projected_crs)\n",
    "    all_drainage_proj = all_drainage.to_crs(projected_crs)\n",
    "\n",
    "    ward_drainage = gpd.overlay(wards_proj, all_drainage_proj, how='intersection', keep_geom_type=False)\n",
    "\n",
    "    if ward_identifier_column in ward_drainage.columns:\n",
    "        ward_id_after_overlay = ward_identifier_column\n",
    "    elif f\"{ward_identifier_column}_1\" in ward_drainage.columns:\n",
    "        ward_id_after_overlay = f\"{ward_identifier_column}_1\"\n",
    "    elif f\"{ward_identifier_column}_2\" in ward_drainage.columns:\n",
    "        ward_id_after_overlay = f\"{ward_identifier_column}_2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot find {ward_identifier_column} column in overlay result. Available columns: {ward_drainage.columns.tolist()}\")\n",
    "    \n",
    "    drainage_lengths_dict = defaultdict(float)\n",
    "\n",
    "    print(\"  - Calculating drainage length for each ward individually...\")\n",
    "\n",
    "    for index, row in ward_drainage.iterrows():\n",
    "        ward_id = row[ward_id_after_overlay]\n",
    "        if isinstance(ward_id, pd.Series):\n",
    "            ward_id = ward_id.iloc[0]\n",
    "        segment_length = row.geometry.length\n",
    "        drainage_lengths_dict[ward_id] += segment_length\n",
    "\n",
    "    drainage_length = pd.Series(drainage_lengths_dict)\n",
    "\n",
    "    ward_areas = wards_proj.set_index(ward_identifier_column).geometry.area\n",
    "\n",
    "    drainage_density = (drainage_length / ward_areas).rename('drainage_density')\n",
    "\n",
    "    if 'drainage_density' in wards.columns:\n",
    "        wards = wards.drop(columns=['drainage_density'])\n",
    "    \n",
    "    wards = wards.join(drainage_density, on=ward_identifier_column)\n",
    "    wards['drainage_density'].fillna(0, inplace=True)\n",
    "    print(\"\u2705 Drainage Density calculated from GPKG.\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Using alternative drainage extraction from Earth Engine...\")\n",
    "    wards['drainage_density'] = 0.01  # Placeholder\n",
    "    print(\"\u2705 Placeholder drainage density set. Implement EE extraction as needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20509227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtracting historical traffic/mobility data from Earth Engine...\")\n",
    "\n",
    "mobility_collection = ee.ImageCollection('GOOGLE/GLOBAL_MOBILITY').filterDate('2020-01-01', '2024-12-31')\n",
    "\n",
    "mobility_image = mobility_collection.select(['transit_stations_percent_change_from_baseline', 'workplaces_percent_change_from_baseline']).mean().clip(aoi)\n",
    "\n",
    "\n",
    "wards['historical_mobility_transit'] = 0  # Placeholder\n",
    "wards['historical_mobility_workplaces'] = 0  # Placeholder\n",
    "\n",
    "print(\"\u2705 Historical mobility data extracted (placeholder).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ce01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProcessing traffic congestion factors...\")\n",
    "\n",
    "wards['traffic_factor'] = wards['historical_mobility_transit'] * 0.5 + wards['historical_mobility_workplaces'] * 0.5\n",
    "wards['traffic_factor'] = wards['traffic_factor'].fillna(0)\n",
    "\n",
    "print(\"\u2705 Traffic factors integrated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExtracting real-time weather data...\")\n",
    "\n",
    "bounds = wards.total_bounds  # [minx, miny, maxx, maxy]\n",
    "lat = (bounds[1] + bounds[3]) / 2  # average of min and max y\n",
    "lon = (bounds[0] + bounds[2]) / 2  # average of min and max x\n",
    "\n",
    "weather_api_key = '737bfada8cc22323afe9f0b4d87f00e2'\n",
    "\n",
    "try:\n",
    "    current_weather_url = f'http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={weather_api_key}&units=metric'\n",
    "    current_response = requests.get(current_weather_url)\n",
    "    current_data = current_response.json()\n",
    "\n",
    "    if current_response.status_code == 200:\n",
    "        current_rain = current_data.get('rain', {}).get('1h', 0)  # rainfall in last hour (mm)\n",
    "        current_temp = current_data['main']['temp']\n",
    "        current_humidity = current_data['main']['humidity']\n",
    "        print(f\"Current weather - Temp: {current_temp}\u00b0C, Humidity: {current_humidity}%, Rain (1h): {current_rain}mm\")\n",
    "    else:\n",
    "        current_rain = 0\n",
    "        current_temp = 25  # default\n",
    "        current_humidity = 60  # default\n",
    "        print(\"\u26a0\ufe0f Could not fetch current weather data\")\n",
    "\n",
    "    forecast_url = f'http://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={weather_api_key}&units=metric'\n",
    "    forecast_response = requests.get(forecast_url)\n",
    "    forecast_data = forecast_response.json()\n",
    "\n",
    "    if forecast_response.status_code == 200:\n",
    "        forecast_rain = []\n",
    "        for item in forecast_data['list'][:40]:  # 5 days * 8 (3-hourly)\n",
    "            rain = item.get('rain', {}).get('3h', 0)\n",
    "            forecast_rain.append(rain)\n",
    "\n",
    "        daily_forecast_rain = [sum(forecast_rain[i:i+8]) for i in range(0, len(forecast_rain), 8)][:5]\n",
    "        print(f\"5-day rainfall forecast: {daily_forecast_rain}\")\n",
    "    else:\n",
    "        daily_forecast_rain = [0] * 5\n",
    "        print(\"\u26a0\ufe0f Could not fetch weather forecast\")\n",
    "\n",
    "    wards['current_rainfall'] = current_rain\n",
    "    wards['current_temperature'] = current_temp\n",
    "    wards['current_humidity'] = current_humidity\n",
    "    for i, rain in enumerate(daily_forecast_rain):\n",
    "        wards[f'forecast_rain_day_{i+1}'] = rain\n",
    "\n",
    "    print(\"\u2705 Real-time weather data integrated\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error fetching weather data: {e}\")\n",
    "    wards['current_rainfall'] = 0\n",
    "    wards['current_temperature'] = 25\n",
    "    wards['current_humidity'] = 60\n",
    "    for i in range(5):\n",
    "        wards[f'forecast_rain_day_{i+1}'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving extracted data...\")\n",
    "\n",
    "wards.to_file('wards_extracted.geojson', driver='GeoJSON')\n",
    "wards.to_pickle('wards_extracted.pkl')\n",
    "\n",
    "print(\"\u2705 Data saved to wards_extracted.geojson and wards_extracted.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}