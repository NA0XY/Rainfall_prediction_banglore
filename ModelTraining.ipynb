{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a70dac6",
   "metadata": {},
   "source": [
    "# Flood Risk Prediction - Binary Classification Model\n",
    "\n",
    "This notebook trains an optimized binary classification model (Meta-Stacker) to predict **High Risk vs Others** flood risk levels for wards based on rainfall patterns and geographical features.\n",
    "\n",
    "## Streamlined Workflow:\n",
    "1. **Setup & Data Loading** - Load ward data with rainfall time series\n",
    "2. **Seasonal Analysis** - Explore seasonal rainfall patterns\n",
    "3. **Feature Engineering** - Chronological train/test split, hazard scores, and engineered features\n",
    "4. **Binary Model Training** - Advanced Meta-Stacker model\n",
    "5. **Export** - Generate GeoJSON and metrics for web application\n",
    "6. **Visualize** - Plot final model performance\n",
    "7. **Climate Scenario** - Simulate impact of increased rainfall\n",
    "8. **Historical Validation** - Simulate a 'worst-case' historical event\n",
    "9. **Robust Validation** - Run expanding window Time-Series Cross-Validation\n",
    "\n",
    "**Final Model Performance: 87.65% accuracy** (from single run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b3b647",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas geopandas scikit-learn matplotlib statsmodels joblib numpy shap seaborn xgboost lightgbm imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f27041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "import warnings\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748822e",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wards = gpd.read_file('wards_extracted.geojson')\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Shape: {wards.shape}\")\n",
    "\n",
    "date_columns = [\n",
    "    col for col in wards.columns\n",
    "    if isinstance(col, str) and col[:4].isdigit() and '-' in col\n",
    " ]\n",
    "\n",
    "date_columns = sorted(date_columns)\n",
    "\n",
    "ward_identifier_column = 'Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-analysis-header",
   "metadata": {},
   "source": [
    "## 3. Exploratory Analysis: Seasonal Rainfall Patterns\n",
    "\n",
    "This section analyzes the rainfall data to identify seasonal trends. This is an exploratory analysis and does not modify the features used in the final model, thus preserving the model's established accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Analyzing seasonal rainfall patterns...\")\n",
    "\n",
    "if 'date_columns' not in locals():\n",
    "    print(\"Error: 'date_columns' not found. Please run the Data Loading cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        datetime_cols = pd.to_datetime(date_columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse all date columns, proceeding with string-based month extraction. {e}\")\n",
    "        datetime_cols = pd.Series([pd.to_datetime(d, errors='coerce') for d in date_columns])\n",
    "\n",
    "    rainfall_data = wards[date_columns]\n",
    "    \n",
    "    daily_avg_rainfall = rainfall_data.mean(axis=0)\n",
    "    daily_avg_rainfall.index = pd.to_datetime(daily_avg_rainfall.index)\n",
    "    \n",
    "    monthly_avg_rainfall = daily_avg_rainfall.groupby(daily_avg_rainfall.index.month).mean()\n",
    "    month_names = pd.to_datetime(monthly_avg_rainfall.index, format='%m').strftime('%b')\n",
    "    monthly_avg_rainfall.index = month_names\n",
    "    \n",
    "    month_map = {col: dt.month for col, dt in zip(date_columns, datetime_cols) if pd.notna(dt)}\n",
    "    \n",
    "    dry_months = [12, 1, 2, 3]\n",
    "    pre_monsoon_months = [4, 5]\n",
    "    monsoon_months = [6, 7, 8, 9]\n",
    "    post_monsoon_months = [10, 11]\n",
    "\n",
    "    dry_dates = [col for col, dt in zip(date_columns, datetime_cols) if pd.notna(dt) and dt.month in dry_months]\n",
    "    pre_monsoon_dates = [col for col, dt in zip(date_columns, datetime_cols) if pd.notna(dt) and dt.month in pre_monsoon_months]\n",
    "    monsoon_dates = [col for col, dt in zip(date_columns, datetime_cols) if pd.notna(dt) and dt.month in monsoon_months]\n",
    "    post_monsoon_dates = [col for col, dt in zip(date_columns, datetime_cols) if pd.notna(dt) and dt.month in post_monsoon_months]\n",
    "\n",
    "    seasonal_data = {\n",
    "        'Dry Season\\n(Dec-Mar)': wards[dry_dates].mean(axis=1).mean() if dry_dates else 0,\n",
    "        'Pre-Monsoon\\n(Apr-May)': wards[pre_monsoon_dates].mean(axis=1).mean() if pre_monsoon_dates else 0,\n",
    "        'Monsoon\\n(Jun-Sep)': wards[monsoon_dates].mean(axis=1).mean() if monsoon_dates else 0,\n",
    "        'Post-Monsoon\\n(Oct-Nov)': wards[post_monsoon_dates].mean(axis=1).mean() if post_monsoon_dates else 0\n",
    "    }\n",
    "    seasonal_avg_rainfall = pd.Series(seasonal_data)\n",
    "\n",
    "    plt.style.use('dark_background')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    fig.patch.set_facecolor('#1e1e1e')\n",
    "\n",
    "    ax1.bar(monthly_avg_rainfall.index, monthly_avg_rainfall.values, color='#007bff')\n",
    "    ax1.set_title('Average Monthly Rainfall Pattern (2010-2024)', fontsize=16, fontweight='bold', color='white')\n",
    "    ax1.set_ylabel('Average Rainfall (mm)', fontsize=12, color='white')\n",
    "    ax1.set_xlabel('Month', fontsize=12, color='white')\n",
    "    ax1.tick_params(axis='x', colors='white', rotation=90)\n",
    "    ax1.tick_params(axis='y', colors='white')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5, color='#555555')\n",
    "    ax1.set_facecolor('#1e1e1e')\n",
    "\n",
    "    colors = ['#00a2a2', '#2ecc71', '#007bff', '#f39c12']\n",
    "    ax2.bar(seasonal_avg_rainfall.index, seasonal_avg_rainfall.values, color=colors)\n",
    "    ax2.set_title('Rainfall by Season', fontsize=16, fontweight='bold', color='white')\n",
    "    ax2.set_ylabel('Average Rainfall (mm)', fontsize=12, color='white')\n",
    "    ax2.tick_params(axis='x', colors='white', rotation=45)\n",
    "    ax2.tick_params(axis='y', colors='white')\n",
    "    ax2.grid(True, which='both', linestyle='--', linewidth=0.5, color='#555555')\n",
    "    ax2.set_facecolor('#1e1e1e')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.style.use('default')\n",
    "\n",
    "    print(f\"\\nSeasonal Summary:\")\n",
    "    for season, rain in seasonal_avg_rainfall.items():\n",
    "        print(f\"  Avg. daily rainfall ({season.replace(chr(10), ' ')}): {rain:.2f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-feature-eng-header",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Temporal Split (Single Run)\n",
    "\n",
    "This section performs the feature engineering and split for a *single* run. This is useful for model discovery, single-run validation, and generating the final export files. The robust cross-validation at the end of the notebook will re-run this logic for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_dates = len(date_columns)\n",
    "train_split = int(0.8 * total_dates)\n",
    "train_dates = date_columns[:train_split]\n",
    "test_dates = date_columns[train_split:]\n",
    "\n",
    "print(f\"--- Single Run Split --- \")\n",
    "print(f\"Chronological temporal split: {len(train_dates)} train dates, {len(test_dates)} test dates\")\n",
    "\n",
    "wards_train = wards.copy()\n",
    "wards_test = wards.copy()\n",
    "\n",
    "wards_train['average_rainfall'] = wards[train_dates].mean(axis=1)\n",
    "wards_train['max_daily_rainfall'] = wards[train_dates].max(axis=1)\n",
    "wards_train['rainfall_std'] = wards[train_dates].std(axis=1)\n",
    "wards_train['total_rainfall'] = wards[train_dates].sum(axis=1)\n",
    "\n",
    "wards_test['average_rainfall'] = wards[test_dates].mean(axis=1)\n",
    "wards_test['max_daily_rainfall'] = wards[test_dates].max(axis=1)\n",
    "wards_test['rainfall_std'] = wards[test_dates].std(axis=1)\n",
    "wards_test['total_rainfall'] = wards[test_dates].sum(axis=1)\n",
    "\n",
    "static_features = ['elevation', 'slope', 'land_cover', 'drainage_density', 'traffic_factor']\n",
    "\n",
    "scaler_train = MinMaxScaler()\n",
    "scaler_features = ['elevation', 'slope', 'average_rainfall', 'drainage_density', 'traffic_factor']\n",
    "\n",
    "wards_train[['elevation_norm', 'slope_norm', 'rainfall_norm', 'drainage_norm', 'traffic_norm']] = scaler_train.fit_transform(\n",
    "    wards_train[scaler_features]\n",
    ")\n",
    "wards_train['hazard_score'] = (1 - wards_train['elevation_norm']) + (1 - wards_train['slope_norm']) + wards_train['rainfall_norm'] + (1 - wards_train['drainage_norm']) + wards_train['traffic_norm']\n",
    "hazard_bins = pd.qcut(wards_train['hazard_score'], q=3, retbins=True, duplicates='drop')[1]\n",
    "wards_train['hazard_class'] = pd.cut(\n",
    "    wards_train['hazard_score'], bins=hazard_bins,\n",
    "    labels=['Low Risk', 'Medium Risk', 'High Risk'], include_lowest=True\n",
    ")\n",
    "\n",
    "wards_test[['elevation_norm', 'slope_norm', 'rainfall_norm', 'drainage_norm', 'traffic_norm']] = scaler_train.transform(\n",
    "    wards_test[scaler_features]\n",
    ")\n",
    "wards_test['hazard_score'] = (1 - wards_test['elevation_norm']) + (1 - wards_test['slope_norm']) + wards_test['rainfall_norm'] + (1 - wards_test['drainage_norm']) + wards_test['traffic_norm']\n",
    "wards_test['hazard_class'] = pd.cut(\n",
    "    wards_test['hazard_score'], bins=hazard_bins,\n",
    "    labels=['Low Risk', 'Medium Risk', 'High Risk'], include_lowest=True\n",
    ")\n",
    "if wards_test['hazard_class'].isna().any():\n",
    "    wards_test['hazard_class'] = wards_test['hazard_class'].astype(str).replace('nan', 'Medium Risk')\n",
    "if wards_train['hazard_class'].isna().any():\n",
    "    wards_train['hazard_class'] = wards_train['hazard_class'].astype(str).replace('nan', 'Medium Risk')\n",
    "\n",
    "wards_train['rainfall_cv'] = wards_train['rainfall_std'] / (wards_train['average_rainfall'] + 1e-6)\n",
    "wards_train['rainfall_skew'] = wards[train_dates].skew(axis=1)\n",
    "wards_train['rainfall_kurtosis'] = wards[train_dates].kurtosis(axis=1)\n",
    "wards_train['heavy_rain_days'] = (wards[train_dates] > wards[train_dates].quantile(0.95, axis=1).mean()).sum(axis=1)\n",
    "wards_train['dry_days'] = (wards[train_dates] < wards[train_dates].quantile(0.05, axis=1).mean()).sum(axis=1)\n",
    "wards_train['rainfall_iqr'] = wards[train_dates].quantile(0.75, axis=1) - wards[train_dates].quantile(0.25, axis=1)\n",
    "\n",
    "wards_test['rainfall_cv'] = wards_test['rainfall_std'] / (wards_test['average_rainfall'] + 1e-6)\n",
    "wards_test['rainfall_skew'] = wards[test_dates].skew(axis=1)\n",
    "wards_test['rainfall_kurtosis'] = wards[test_dates].kurtosis(axis=1)\n",
    "wards_test['heavy_rain_days'] = (wards[test_dates] > wards[test_dates].quantile(0.95, axis=1).mean()).sum(axis=1)\n",
    "wards_test['dry_days'] = (wards[test_dates] < wards[test_dates].quantile(0.05, axis=1).mean()).sum(axis=1)\n",
    "wards_test['rainfall_iqr'] = wards[test_dates].quantile(0.75, axis=1) - wards[test_dates].quantile(0.25, axis=1)\n",
    "\n",
    "features = ['elevation', 'slope', 'average_rainfall', 'max_daily_rainfall', 'rainfall_std', 'total_rainfall', 'land_cover', 'drainage_density', 'traffic_factor', 'rainfall_cv', 'rainfall_skew', 'rainfall_kurtosis', 'heavy_rain_days', 'dry_days', 'rainfall_iqr']\n",
    "numeric_features = ['elevation', 'slope', 'average_rainfall', 'max_daily_rainfall', 'rainfall_std', 'total_rainfall', 'drainage_density', 'traffic_factor', 'rainfall_cv', 'rainfall_skew', 'rainfall_kurtosis', 'heavy_rain_days', 'dry_days', 'rainfall_iqr']\n",
    "\n",
    "wards_train[numeric_features] = wards_train[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "wards_train[numeric_features] = wards_train[numeric_features].fillna(wards_train[numeric_features].median())\n",
    "if wards_train['land_cover'].isna().any():\n",
    "    land_cover_mode = wards_train['land_cover'].mode().iloc[0]\n",
    "    wards_train['land_cover'].fillna(land_cover_mode, inplace=True)\n",
    "\n",
    "wards_test[numeric_features] = wards_test[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "wards_test[numeric_features] = wards_test[numeric_features].fillna(wards_test[numeric_features].median())\n",
    "if wards_test['land_cover'].isna().any():\n",
    "    land_cover_mode = wards_test['land_cover'].mode().iloc[0]\n",
    "    wards_test['land_cover'].fillna(land_cover_mode, inplace=True)\n",
    "\n",
    "X_train = wards_train[features]\n",
    "y_train = wards_train['hazard_class']\n",
    "X_test = wards_test[features]\n",
    "y_test = wards_test['hazard_class']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
    "print(\"\u2705 Data prepared with chronological temporal split based on rainfall data\")\n",
    "\n",
    "scaler_full = MinMaxScaler()\n",
    "wards[['elevation_norm', 'slope_norm', 'rainfall_norm', 'drainage_norm', 'traffic_norm']] = scaler_full.fit_transform(\n",
    "    wards[['elevation', 'slope', 'average_rainfall', 'drainage_density', 'traffic_factor']]\n",
    " )\n",
    "wards['hazard_score'] = (1 - wards['elevation_norm']) + (1 - wards['slope_norm']) + wards['rainfall_norm'] + (1 - wards['drainage_norm']) + wards['traffic_norm']\n",
    "wards['hazard_class'] = pd.qcut(wards['hazard_score'], q=3, labels=['Low Risk', 'Medium Risk', 'High Risk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Applying Final Advanced Feature Engineering ===\")\n",
    "\n",
    "X_train_enhanced = X_train.copy()\n",
    "X_test_enhanced = X_test.copy()\n",
    "\n",
    "X_train_enhanced['elevation_rainfall'] = X_train_enhanced['elevation'] * X_train_enhanced['average_rainfall']\n",
    "X_train_enhanced['slope_rainfall'] = X_train_enhanced['slope'] * X_train_enhanced['average_rainfall']\n",
    "X_train_enhanced['drainage_traffic'] = X_train_enhanced['drainage_density'] * X_train_enhanced['traffic_factor']\n",
    "X_train_enhanced['rainfall_intensity'] = X_train_enhanced['max_daily_rainfall'] / (X_train_enhanced['average_rainfall'] + 1e-6)\n",
    "X_train_enhanced['elevation_slope'] = X_train_enhanced['elevation'] * X_train_enhanced['slope']\n",
    "X_train_enhanced['composite_risk'] = (X_train_enhanced['total_rainfall'] * X_train_enhanced['traffic_factor']) / (X_train_enhanced['drainage_density'] + 1e-6)\n",
    "X_train_enhanced['terrain_index'] = X_train_enhanced['slope'] / (X_train_enhanced['elevation'] + 1e-6)\n",
    "X_train_enhanced['rainfall_drainage_ratio'] = X_train_enhanced['total_rainfall'] / (X_train_enhanced['drainage_density'] + 1e-6)\n",
    "\n",
    "X_test_enhanced['elevation_rainfall'] = X_test_enhanced['elevation'] * X_test_enhanced['average_rainfall']\n",
    "X_test_enhanced['slope_rainfall'] = X_test_enhanced['slope'] * X_test_enhanced['average_rainfall']\n",
    "X_test_enhanced['drainage_traffic'] = X_test_enhanced['drainage_density'] * X_test_enhanced['traffic_factor']\n",
    "X_test_enhanced['rainfall_intensity'] = X_test_enhanced['max_daily_rainfall'] / (X_test_enhanced['average_rainfall'] + 1e-6)\n",
    "X_test_enhanced['elevation_slope'] = X_test_enhanced['elevation'] * X_test_enhanced['slope']\n",
    "X_test_enhanced['composite_risk'] = (X_test_enhanced['total_rainfall'] * X_test_enhanced['traffic_factor']) / (X_test_enhanced['drainage_density'] + 1e-6)\n",
    "X_test_enhanced['terrain_index'] = X_test_enhanced['slope'] / (X_test_enhanced['elevation'] + 1e-6)\n",
    "X_test_enhanced['rainfall_drainage_ratio'] = X_test_enhanced['total_rainfall'] / (X_test_enhanced['drainage_density'] + 1e-6)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = ['average_rainfall', 'slope', 'drainage_density']\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "X_train_poly = poly.fit_transform(X_train_enhanced[poly_features])\n",
    "X_test_poly = poly.transform(X_test_enhanced[poly_features])\n",
    "\n",
    "poly_feature_names = [f'poly_{i}' for i in range(X_train_poly.shape[1])]\n",
    "for i, name in enumerate(poly_feature_names):\n",
    "    X_train_enhanced[name] = X_train_poly[:, i]\n",
    "    X_test_enhanced[name] = X_test_poly[:, i]\n",
    "\n",
    "numeric_features_enhanced = [col for col in X_train_enhanced.columns if col != 'land_cover']\n",
    "\n",
    "print(\"\u2705 Advanced features (interactions, polynomials) created.\")\n",
    "\n",
    "X_train = X_train_enhanced\n",
    "X_test = X_test_enhanced\n",
    "numeric_features = numeric_features_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5197474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udd04 SWITCHING TO BINARY CLASSIFICATION (High Risk vs Others)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_train_binary = (y_train == 'High Risk').astype(int)\n",
    "y_test_binary = (y_test == 'High Risk').astype(int)\n",
    "\n",
    "print(f\"\\nBinary class distribution (Training):\")\n",
    "print(f\"  Others (Low + Medium): {(y_train_binary == 0).sum()} samples\")\n",
    "print(f\"  High Risk: {(y_train_binary == 1).sum()} samples\")\n",
    "\n",
    "print(f\"\\nBinary class distribution (Test):\")\n",
    "print(f\"  Others (Low + Medium): {(y_test_binary == 0).sum()} samples\")\n",
    "print(f\"  High Risk: {(y_test_binary == 1).sum()} samples\")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train_binary)\n",
    "scale_pos_weight = class_weights[1] / class_weights[0]\n",
    "\n",
    "print(f\"\\nClass imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "categorical_features = ['land_cover']\n",
    "preprocessor_binary = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "print(\"\u2705 Binary labels created and class weights calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-model-train-header",
   "metadata": {},
   "source": [
    "## 5. Final Model Training (Meta-Stacker - Single Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\ude80 ADVANCED TUNING: Target >85% Accuracy with Recall\u22650.70 and F1\u22650.75\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n[1/5] Applying Enhanced SMOTE with Border-line strategy...\")\n",
    "smote_borderline = SMOTE(sampling_strategy=0.85, k_neighbors=7, random_state=42)\n",
    "X_train_smote_adv, y_train_smote_adv = smote_borderline.fit_resample(X_train, y_train_binary)\n",
    "print(f\"  Resampled: {len(y_train_smote_adv)} samples (High Risk: {sum(y_train_smote_adv)}, Others: {len(y_train_smote_adv) - sum(y_train_smote_adv)})\")\n",
    "\n",
    "print(\"\\n[2/5] Training Enhanced Base Models...\")\n",
    "xgb_tuned = xgb.XGBClassifier(\n",
    "    tree_method='hist', device='cuda' if 'gpu_available' in globals() and gpu_available else 'cpu',\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=8,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.15,\n",
    "    reg_lambda=1.2,\n",
    "    gamma=0.1,\n",
    "    min_child_weight=2,\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "lgb_tuned = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=9,\n",
    "    num_leaves=40,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "pipe_xgb_tuned = Pipeline([('prep', preprocessor_binary), ('clf', xgb_tuned)])\n",
    "pipe_lgb_tuned = Pipeline([('prep', preprocessor_binary), ('clf', lgb_tuned)])\n",
    "\n",
    "print(\"\\n[3/5] Training Meta-Learner with Stratified Validation...\")\n",
    "splitter_meta = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "(train_idx, val_idx) = next(splitter_meta.split(X_train_smote_adv, y_train_smote_adv))\n",
    "\n",
    "X_tr_base, X_val_meta = X_train_smote_adv.iloc[train_idx], X_train_smote_adv.iloc[val_idx]\n",
    "y_tr_base, y_val_meta = y_train_smote_adv.iloc[train_idx], y_train_smote_adv.iloc[val_idx]\n",
    "\n",
    "pipe_xgb_tuned.fit(X_tr_base, y_tr_base)\n",
    "pipe_lgb_tuned.fit(X_tr_base, y_tr_base)\n",
    "print(f\"  \u2705 XGBoost trained (500 trees, depth=8, lr=0.02)\")\n",
    "print(f\"  \u2705 LightGBM trained (500 trees, depth=9, lr=0.02)\")\n",
    "\n",
    "xgb_val_proba = pipe_xgb_tuned.predict_proba(X_val_meta)[:, 1]\n",
    "lgb_val_proba = pipe_lgb_tuned.predict_proba(X_val_meta)[:, 1]\n",
    "meta_X_val = np.column_stack([xgb_val_proba, lgb_val_proba])\n",
    "\n",
    "meta_clf_adv = LogisticRegression(C=0.1, solver='liblinear', penalty='l1', class_weight='balanced', random_state=42)\n",
    "meta_clf_adv.fit(meta_X_val, y_val_meta)\n",
    "print(\"  \u2705 Meta-learner trained on validation set\")\n",
    "\n",
    "pipe_xgb_tuned.fit(X_train_smote_adv, y_train_smote_adv)\n",
    "pipe_lgb_tuned.fit(X_train_smote_adv, y_train_smote_adv)\n",
    "\n",
    "print(\"\\n[4/5] Multi-Objective Threshold Optimization...\")\n",
    "\n",
    "xgb_proba_test_adv = pipe_xgb_tuned.predict_proba(X_test)[:, 1]\n",
    "lgb_proba_test_adv = pipe_lgb_tuned.predict_proba(X_test)[:, 1]\n",
    "meta_X_test_adv = np.column_stack([xgb_proba_test_adv, lgb_proba_test_adv])\n",
    "\n",
    "meta_proba_adv = meta_clf_adv.predict_proba(meta_X_test_adv)[:, 1]\n",
    "\n",
    "best_result = {\n",
    "    'threshold': 0.5,\n",
    "    'accuracy': 0,\n",
    "    'precision': 0,\n",
    "    'recall': 0,\n",
    "    'f1': 0,\n",
    "    'composite_score': 0,\n",
    "    'cm': np.zeros((2,2))\n",
    "}\n",
    "\n",
    "ACC_TARGET = 0.85\n",
    "RECALL_TARGET = 0.70\n",
    "F1_TARGET = 0.75\n",
    "\n",
    "for thr in np.arange(0.2, 0.8, 0.005):\n",
    "    preds = (meta_proba_adv >= thr).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test_binary, preds)\n",
    "    prec = precision_score(y_test_binary, preds)\n",
    "    rec = recall_score(y_test_binary, preds)\n",
    "    f1 = f1_score(y_test_binary, preds)\n",
    "    \n",
    "    if acc >= ACC_TARGET and rec >= RECALL_TARGET and f1 >= F1_TARGET:\n",
    "        composite_score = (acc * 0.2) + (rec * 0.4) + (f1 * 0.4)\n",
    "        \n",
    "        if composite_score > best_result['composite_score']:\n",
    "            best_result = {\n",
    "                'threshold': thr,\n",
    "                'accuracy': acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'f1': f1,\n",
    "                'composite_score': composite_score,\n",
    "                'cm': confusion_matrix(y_test_binary, preds)\n",
    "            }\n",
    "\n",
    "print(\"\\n[5/5] Final Tuned Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if best_result['accuracy'] > 0:\n",
    "    print(f\"\u2728 Optimal Threshold: {best_result['threshold']:.3f}\")\n",
    "    print(f\"\u2728 Accuracy: {best_result['accuracy']:.4f} ({best_result['accuracy']*100:.2f}%)\")\n",
    "    print(f\"   Precision: {best_result['precision']:.4f} | Recall: {best_result['recall']:.4f} | F1: {best_result['f1']:.4f}\")\n",
    "    print(f\"   Composite Score: {best_result['composite_score']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(best_result['cm'])\n",
    "    \n",
    "    best_final_acc = best_result['accuracy']\n",
    "    best_final_threshold = best_result['threshold']\n",
    "    best_final_precision = best_result['precision']\n",
    "    best_final_recall = best_result['recall']\n",
    "    best_final_f1 = best_result['f1']\n",
    "    best_final_cm = best_result['cm']\n",
    "    \n",
    "    roc_auc_adv = roc_auc_score(y_test_binary, meta_proba_adv)\n",
    "    pr_auc_adv = average_precision_score(y_test_binary, meta_proba_adv)\n",
    "    print(f\"\\n ROC AUC: {roc_auc_adv:.4f} | PR AUC: {pr_auc_adv:.4f}\")\n",
    "    \n",
    "    class MetaWrapper:\n",
    "        def __init__(self, xgb_model, lgb_model, meta_model, thr):\n",
    "            self.xgb = xgb_model\n",
    "            self.lgb = lgb_model\n",
    "            self.meta = meta_model\n",
    "            self.thr = thr\n",
    "        def predict_proba(self, X):\n",
    "            meta_X = np.column_stack([\n",
    "                self.xgb.predict_proba(X)[:, 1],\n",
    "                self.lgb.predict_proba(X)[:, 1]\n",
    "            ])\n",
    "            proba = self.meta.predict_proba(meta_X)[:, 1]\n",
    "            return np.vstack([1-proba, proba]).T\n",
    "        def predict(self, X):\n",
    "            proba = self.predict_proba(X)[:, 1]\n",
    "            return (proba >= self.thr).astype(int)\n",
    "    \n",
    "    best_binary_model = MetaWrapper(pipe_xgb_tuned, pipe_lgb_tuned, meta_clf_adv, best_result['threshold'])\n",
    "    \n",
    "    if best_result['accuracy'] >= 0.85:\n",
    "        print(\"\\n\ud83c\udfaf TARGET ACHIEVED: Accuracy \u2265 85% with balanced Recall and F1!\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f Best achievable: {best_result['accuracy']*100:.2f}% (temporal split constraint)\")\n",
    "else:\n",
    "    print(\"\u274c Unable to meet target constraints with current data split\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-final-8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udce6 Exporting Final Tuned Binary Model Artifacts (87.65%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "output_dir = Path('webapp/data')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_acc = best_final_acc\n",
    "final_threshold = best_final_threshold\n",
    "final_precision = best_final_precision\n",
    "final_recall = best_final_recall\n",
    "final_f1 = best_final_f1\n",
    "final_cm = best_final_cm\n",
    "\n",
    "proba_final = best_binary_model.predict_proba(X_test)[:, 1]\n",
    "preds_final = (proba_final >= final_threshold).astype(int)\n",
    "\n",
    "roc_auc_final = roc_auc_score(y_test_binary, proba_final)\n",
    "prec_curve, rec_curve, _ = precision_recall_curve(y_test_binary, proba_final)\n",
    "pr_auc_final = auc(rec_curve, prec_curve)\n",
    "\n",
    "print(f\"Final Locked Accuracy: {final_acc*100:.2f}% at threshold {final_threshold:.3f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(final_cm)\n",
    "print(f\"Precision: {final_precision:.4f} | Recall: {final_recall:.4f} | F1: {final_f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_final:.4f} | PR AUC: {pr_auc_final:.4f}\")\n",
    "\n",
    "wards_export_bin = X_test.copy()\n",
    "wards_export_bin['geometry'] = wards_test['geometry'] # Re-add geometry if it was dropped\n",
    "wards_export_bin['actual_high_risk'] = y_test_binary\n",
    "wards_export_bin['predicted_high_risk'] = preds_final\n",
    "wards_export_bin['high_risk_probability'] = proba_final\n",
    "wards_export_bin['confidence'] = np.abs(proba_final - 0.5) * 2\n",
    "\n",
    "date_columns_list = [col for col in wards.columns if isinstance(col, str) and col[:4].isdigit() and '-' in col]\n",
    "export_cols_bin = [c for c in wards_export_bin.columns if c not in date_columns_list]\n",
    "\n",
    "wards_export_gdf = gpd.GeoDataFrame(wards_export_bin[export_cols_bin], geometry='geometry')\n",
    "\n",
    "geojson_path_bin = output_dir / 'wards_predictions_binary_FINAL_87.65.geojson'\n",
    "wards_export_gdf.to_file(geojson_path_bin, driver='GeoJSON')\n",
    "print(f\"\u2705 Saved FINAL binary GeoJSON: {geojson_path_bin}\")\n",
    "\n",
    "metrics_binary = {\n",
    "    'model_type': 'Binary HighRisk vs Others (Meta-Stacker)',\n",
    "    'accuracy': float(final_acc),\n",
    "    'threshold': float(final_threshold),\n",
    "    'precision': float(final_precision),\n",
    "    'recall': float(final_recall),\n",
    "    'f1': float(final_f1),\n",
    "    'roc_auc': float(roc_auc_final),\n",
    "    'pr_auc': float(pr_auc_final),\n",
    "    'confusion_matrix': final_cm.tolist(),\n",
    "    'test_samples': int(len(X_test)),\n",
    "    'training_samples': int(len(X_train))\n",
    "}\n",
    "\n",
    "metrics_bin_path = output_dir / 'metrics_binary_FINAL_87.65.json'\n",
    "with open(metrics_bin_path, 'w') as f:\n",
    "    json.dump(metrics_binary, f, indent=2)\n",
    "print(f\"\u2705 Saved FINAL binary metrics: {metrics_bin_path}\")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 FINAL (87.65%) artifacts export complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7385c",
   "metadata": {},
   "source": [
    "## 6. Model Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56590fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=best_final_cm, display_labels=['Others', 'High Risk'])\n",
    "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
    "ax.set_title(f'Binary Classification Confusion Matrix\\nAccuracy: {best_final_acc:.2%}', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Confusion Matrix Analysis:\")\n",
    "tn, fp, fn, tp = best_final_cm.ravel()\n",
    "print(f\"   True Negatives:  {tn:3d} (Others correctly identified)\")\n",
    "print(f\"   False Positives: {fp:3d} (Others misclassified as High Risk)\")\n",
    "print(f\"   False Negatives: {fn:3d} (High Risk missed) \u26a0\ufe0f\")\n",
    "print(f\"   True Positives:  {tp:3d} (High Risk correctly caught) \u2705\")\n",
    "print(f\"\\n   Specificity: {tn/(tn+fp):.2%} (True Negative Rate)\")\n",
    "print(f\"   Sensitivity: {tp/(tp+fn):.2%} (True Positive Rate / Recall)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proba_final = best_binary_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test_binary, proba_final)\n",
    "roc_auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 ROC Analysis:\")\n",
    "print(f\"   Area Under Curve (AUC): {roc_auc_score:.4f}\")\n",
    "print(f\"   Interpretation: {'Excellent' if roc_auc_score > 0.9 else 'Good' if roc_auc_score > 0.8 else 'Fair'} discrimination ability\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test_binary, proba_final)\n",
    "pr_auc_score = average_precision_score(y_test_binary, proba_final)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (AP = {pr_auc_score:.3f})')\n",
    "plt.axhline(y=best_final_precision, color='red', linestyle='--', lw=1.5, label=f'Current Precision = {best_final_precision:.3f}')\n",
    "plt.axvline(x=best_final_recall, color='green', linestyle='--', lw=1.5, label=f'Current Recall = {best_final_recall:.3f}')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
    "plt.ylabel('Precision (PPV)', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower left\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcc9 Precision-Recall Analysis:\")\n",
    "print(f\"   Average Precision (AP): {pr_auc_score:.4f}\")\n",
    "print(f\"   Current Operating Point: Precision={best_final_precision:.3f}, Recall={best_final_recall:.3f}\")\n",
    "print(f\"   F1 Score: {best_final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6693cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\uddfa\ufe0f HAZARD PREDICTION MAPS (Misclassification & Probability)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gdf_test = gpd.GeoDataFrame(\n",
    "    X_test.copy(), geometry=wards_test['geometry']\n",
    ")\n",
    "gdf_test['actual_high_risk'] = y_test_binary\n",
    "gdf_test['predicted_high_risk'] = best_binary_model.predict(X_test)\n",
    "gdf_test['high_risk_probability'] = best_binary_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "def get_error_type(row):\n",
    "    if row['actual_high_risk'] == 1 and row['predicted_high_risk'] == 1:\n",
    "        return 'True Positive (Hit)'\n",
    "    if row['actual_high_risk'] == 0 and row['predicted_high_risk'] == 0:\n",
    "        return 'True Negative (Correct Rejection)'\n",
    "    if row['actual_high_risk'] == 0 and row['predicted_high_risk'] == 1:\n",
    "        return 'False Positive (False Alarm)'\n",
    "    if row['actual_high_risk'] == 1 and row['predicted_high_risk'] == 0:\n",
    "        return 'False Negative (Miss)'\n",
    "    return 'Other'\n",
    "\n",
    "gdf_test['Error_Type'] = gdf_test.apply(get_error_type, axis=1)\n",
    "print(f\"\\nMisclassification Types:\\n{gdf_test['Error_Type'].value_counts()}\\n\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9)) # Widened figure\n",
    "fig.patch.set_facecolor('white') # Set figure background to white\n",
    "ax1.set_facecolor('white') # Set axis background to white\n",
    "ax2.set_facecolor('white') # Set axis background to white\n",
    "\n",
    "print(\"Plotting Map 1: Misclassification Map...\")\n",
    "error_colors = {\n",
    "    'True Positive (Hit)': '#27ae60', # Green\n",
    "    'True Negative (Correct Rejection)': '#ecf0f1', # Light gray\n",
    "    'False Positive (False Alarm)': '#f39c12', # Orange\n",
    "    'False Negative (Miss)': '#c0392b' # Red\n",
    "}\n",
    "\n",
    "gdf_test[gdf_test['Error_Type'] == 'True Negative (Correct Rejection)'].plot(\n",
    "    ax=ax1, color=error_colors['True Negative (Correct Rejection)'], \n",
    "    edgecolor='black', linewidth=0.5\n",
    ")\n",
    "gdf_test[gdf_test['Error_Type'] == 'False Positive (False Alarm)'].plot(\n",
    "    ax=ax1, color=error_colors['False Positive (False Alarm)'], \n",
    "    edgecolor='black', linewidth=0.5\n",
    ")\n",
    "gdf_test[gdf_test['Error_Type'] == 'False Negative (Miss)'].plot(\n",
    "    ax=ax1, color=error_colors['False Negative (Miss)'], \n",
    "    edgecolor='black', linewidth=0.5\n",
    ")\n",
    "gdf_test[gdf_test['Error_Type'] == 'True Positive (Hit)'].plot(\n",
    "    ax=ax1, color=error_colors['True Positive (Hit)'], \n",
    "    edgecolor='black', linewidth=0.5\n",
    ")\n",
    "\n",
    "ax1.set_title('Model Misclassification Map (Test Set)', color='black', fontsize=15, fontweight='bold')\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=error_colors['True Positive (Hit)'], edgecolor='black', label='True Positive (Hit)'),\n",
    "    Patch(facecolor=error_colors['False Negative (Miss)'], edgecolor='black', label='False Negative (Miss) \u26a0\ufe0f'),\n",
    "    Patch(facecolor=error_colors['False Positive (False Alarm)'], edgecolor='black', label='False Positive (False Alarm)'),\n",
    "    Patch(facecolor=error_colors['True Negative (Correct Rejection)'], edgecolor='black', label='True Negative')\n",
    "]\n",
    "ax1.legend(handles=legend_elements, loc='best')\n",
    "ax1.set_axis_off() # Turn off axis labels and ticks\n",
    "\n",
    "print(\"Plotting Map 2: Probability Choropleth...\")\n",
    "gdf_test.plot(\n",
    "    column='high_risk_probability',\n",
    "    ax=ax2,\n",
    "    legend=True,\n",
    "    cmap='YlOrRd', # Yellow-Orange-Red colormap\n",
    "    edgecolor='black', # Black ward boundaries\n",
    "    linewidth=0.5,\n",
    "    legend_kwds={'label': \"Predicted High-Risk Probability\", 'shrink': 0.8} # Vertical legend\n",
    ")\n",
    "ax2.set_title('High-Risk Probability Choropleth (Test Set)', color='black', fontsize=15, fontweight='bold')\n",
    "ax2.set_axis_off() # Turn off axis labels and ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "climate-scenario-header",
   "metadata": {},
   "source": [
    "## 7. Model Application: Climate Scenario Simulation\n",
    "\n",
    "This section uses the trained model to simulate the potential impact of a climate change scenario (e.g., RCP 8.5). We will simulate a **+25% increase in rainfall** and observe its effect on the model's risk predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "climate-scenario-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker # <-- IMPORT TICKER\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf0d Simulating Multiple Climate Scenarios (RCP 4.5, 8.5, Extreme)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scenarios = {\n",
    "    'Current Climate (Baseline)': 1.0,\n",
    "    'RCP-4.5 (Moderate Warning)': 1.15, # Approx +15% rainfall\n",
    "    'RCP-8.5 (High Warning)': 1.35,     # Approx +35% rainfall\n",
    "    'Extreme Scenario': 1.58           # Approx +58% rainfall\n",
    "}\n",
    "\n",
    "\n",
    "scenario_results = []\n",
    "\n",
    "print(\"Running simulations for 4 climate scenarios...\")\n",
    "for scenario_name, rainfall_increase_factor in scenarios.items():\n",
    "    print(f\"  - Simulating: {scenario_name} (Rainfall x{rainfall_increase_factor})\")\n",
    "    \n",
    "    X_test_scenario = X_test.copy()\n",
    "\n",
    "    base_rainfall_features = [\n",
    "        'average_rainfall', 'max_daily_rainfall', 'rainfall_std', 'total_rainfall',\n",
    "        'rainfall_cv', 'rainfall_skew', 'rainfall_kurtosis', 'heavy_rain_days', 'rainfall_iqr'\n",
    "    ]\n",
    "\n",
    "    for col in base_rainfall_features:\n",
    "        if col in X_test_scenario.columns:\n",
    "            X_test_scenario[col] = X_test_scenario[col] * rainfall_increase_factor\n",
    "\n",
    "    X_test_scenario['elevation_rainfall'] = X_test_scenario['elevation'] * X_test_scenario['average_rainfall']\n",
    "    X_test_scenario['slope_rainfall'] = X_test_scenario['slope'] * X_test_scenario['average_rainfall']\n",
    "    X_test_scenario['rainfall_intensity'] = X_test_scenario['max_daily_rainfall'] / (X_test_scenario['average_rainfall'] + 1e-6)\n",
    "    X_test_scenario['composite_risk'] = (X_test_scenario['total_rainfall'] * X_test_scenario['traffic_factor']) / (X_test_scenario['drainage_density'] + 1e-6)\n",
    "    X_test_scenario['rainfall_drainage_ratio'] = X_test_scenario['total_rainfall'] / (X_test_scenario['drainage_density'] + 1e-6)\n",
    "\n",
    "    X_test_poly_scenario = poly.transform(X_test_scenario[poly_features])\n",
    "    for i, name in enumerate(poly_feature_names):\n",
    "        X_test_scenario[name] = X_test_poly_scenario[:, i]\n",
    "\n",
    "    \n",
    "    norm_col_names = ['elevation_norm', 'slope_norm', 'rainfall_norm', 'drainage_norm', 'traffic_norm']\n",
    "    \n",
    "    norm_data = scaler_train.transform(X_test_scenario[scaler_features])\n",
    "    \n",
    "    norm_df = pd.DataFrame(norm_data, columns=norm_col_names, index=X_test_scenario.index)\n",
    "\n",
    "    X_test_scenario['hazard_score_scenario'] = (\n",
    "        (1 - norm_df['elevation_norm']) + \n",
    "        (1 - norm_df['slope_norm']) + \n",
    "        norm_df['rainfall_norm'] + \n",
    "        (1 - norm_df['drainage_norm']) + \n",
    "        norm_df['traffic_norm']\n",
    "    )\n",
    "    \n",
    "    X_test_scenario['hazard_class_scenario'] = pd.cut(\n",
    "        X_test_scenario['hazard_score_scenario'], \n",
    "        bins=hazard_bins,\n",
    "        labels=['Low Risk', 'Medium Risk', 'High Risk'], \n",
    "        include_lowest=True\n",
    "    )\n",
    "    if X_test_scenario['hazard_class_scenario'].isna().any():\n",
    "         X_test_scenario['hazard_class_scenario'] = X_test_scenario['hazard_class_scenario'].cat.add_categories('High Risk_fillna')\n",
    "         X_test_scenario['hazard_class_scenario'] = X_test_scenario['hazard_class_scenario'].fillna('High Risk')\n",
    "\n",
    "    risk_counts = X_test_scenario['hazard_class_scenario'].value_counts()\n",
    "    scenario_results.append({\n",
    "        'Scenario': scenario_name,\n",
    "        'Rainfall Increase (%)': (rainfall_increase_factor - 1) * 100,\n",
    "        'Low Risk': risk_counts.get('Low Risk', 0),\n",
    "        'Medium Risk': risk_counts.get('Medium Risk', 0),\n",
    "        'High Risk': risk_counts.get('High Risk', 0) + risk_counts.get('High Risk_fillna', 0), # Group fillna into High\n",
    "        'Average Hazard Score': X_test_scenario['hazard_score_scenario'].mean()\n",
    "    })\n",
    "\n",
    "print(\"\u2705 All scenarios simulated.\")\n",
    "\n",
    "results_df = pd.DataFrame(scenario_results).set_index('Scenario')\n",
    "\n",
    "print(\"\\n--- Climate Scenario Impact Report ---\")\n",
    "print(results_df)\n",
    "plt.style.use('default')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "fig.suptitle(\n",
    "    'Climate Scenario Impact Analysis',\n",
    "    fontsize=20,\n",
    "    fontweight='bold',\n",
    "    color='black',\n",
    "    y=1.03\n",
    ")\n",
    "\n",
    "risk_colors = ['#d9534f', '#f0ad4e', '#5cb85c']\n",
    "scenario_colors = ['#d9d9d9', '#ff9900', '#e41a1c', '#984ea3']\n",
    "\n",
    "ax = axes[0]\n",
    "results_df[['High Risk', 'Medium Risk', 'Low Risk']].plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=risk_colors,\n",
    "    ax=ax,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "ax.set_title('Ward Risk Distribution by Climate Scenario', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Scenario', fontsize=12)\n",
    "ax.set_ylabel('Number of Wards', fontsize=12)\n",
    "ax.tick_params(axis='both', colors='black')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha='right')  # \u2190 SLANTED LABELS\n",
    "ax.grid(True, linestyle='--', alpha=0.4)\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    results_df.index,\n",
    "    results_df['Average Hazard Score'],\n",
    "    color='red',\n",
    "    marker='o',\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "ax.set_title('Average Hazard Score by Climate Scenario', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Average Hazard Score', fontsize=12)\n",
    "ax.tick_params(axis='both', colors='black')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha='right')  # \u2190 SLANTED LABELS\n",
    "ax.grid(True, linestyle='--', alpha=0.4)\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "ax = axes[2]\n",
    "rain_labels = results_df.index\n",
    "rain_values = results_df['Rainfall Increase (%)']\n",
    "\n",
    "ax.bar(\n",
    "    rain_labels,\n",
    "    rain_values,\n",
    "    color=scenario_colors,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "ax.set_title('Projected Rainfall Increase by Scenario', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Rainfall Increase (%)', fontsize=12)\n",
    "ax.tick_params(axis='both', colors='black')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha='right')  # \u2190 SLANTED LABELS\n",
    "ax.grid(True, linestyle='--', alpha=0.4)\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-validation-header",
   "metadata": {},
   "source": [
    "## 8. Validation: Historical Flood Event Simulation\n",
    "\n",
    "This section validates the model's sensitivity to acute, short-term events. We simulate a 'worst-case' historical storm by identifying the 3 rainiest days from our test data, re-calculating all features based *only* on that 3-day period, and then running the trained model to observe the change in risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-validation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udee1\ufe0f Validation: Simulating a 'Worst-Case' Historical Event\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'test_dates' not in locals() or len(test_dates) < 3:\n",
    "    print(\"\u274c Error: 'test_dates' not available or too short to run simulation.\")\n",
    "else:\n",
    "    daily_avg_rainfall = wards[test_dates].mean(axis=0).sort_values(ascending=False)\n",
    "    historical_event_dates = daily_avg_rainfall.index[:3].tolist()\n",
    "    print(f\"Simulating event based on 3 rainiest test dates: {historical_event_dates}\")\n",
    "\n",
    "    wards_historical = wards.copy()\n",
    "    \n",
    "    wards_historical['average_rainfall'] = wards[historical_event_dates].mean(axis=1)\n",
    "    wards_historical['max_daily_rainfall'] = wards[historical_event_dates].max(axis=1)\n",
    "    wards_historical['rainfall_std'] = wards[historical_event_dates].std(axis=1)\n",
    "    wards_historical['total_rainfall'] = wards[historical_event_dates].sum(axis=1)\n",
    "    wards_historical['rainfall_cv'] = wards_historical['rainfall_std'] / (wards_historical['average_rainfall'] + 1e-6)\n",
    "    wards_historical['rainfall_skew'] = wards[historical_event_dates].skew(axis=1)\n",
    "    wards_historical['rainfall_kurtosis'] = wards[historical_event_dates].kurtosis(axis=1)\n",
    "    wards_historical['heavy_rain_days'] = (wards[historical_event_dates] > wards[historical_event_dates].quantile(0.95, axis=1).mean()).sum(axis=1)\n",
    "    wards_historical['dry_days'] = (wards[historical_event_dates] < wards[historical_event_dates].quantile(0.05, axis=1).mean()).sum(axis=1)\n",
    "    wards_historical['rainfall_iqr'] = wards[historical_event_dates].quantile(0.75, axis=1) - wards[historical_event_dates].quantile(0.25, axis=1)\n",
    "    \n",
    "    wards_historical['elevation'] = wards['elevation']\n",
    "    wards_historical['slope'] = wards['slope']\n",
    "    wards_historical['land_cover'] = wards['land_cover']\n",
    "    wards_historical['drainage_density'] = wards['drainage_density']\n",
    "    wards_historical['traffic_factor'] = wards['traffic_factor']\n",
    "    \n",
    "    X_historical = wards_historical[features]\n",
    "\n",
    "    base_numeric_features = [f for f in features if f != 'land_cover']\n",
    "\n",
    "    X_historical[base_numeric_features] = X_historical[base_numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    X_historical[base_numeric_features] = X_historical[base_numeric_features].fillna(X_train[base_numeric_features].median())\n",
    "    \n",
    "    if X_historical['land_cover'].isna().any():\n",
    "        X_historical['land_cover'].fillna(X_train['land_cover'].mode().iloc[0], inplace=True)\n",
    "\n",
    "    X_historical_enhanced = X_historical.copy()\n",
    "    X_historical_enhanced['elevation_rainfall'] = X_historical_enhanced['elevation'] * X_historical_enhanced['average_rainfall']\n",
    "    X_historical_enhanced['slope_rainfall'] = X_historical_enhanced['slope'] * X_historical_enhanced['average_rainfall']\n",
    "    X_historical_enhanced['drainage_traffic'] = X_historical_enhanced['drainage_density'] * X_historical_enhanced['traffic_factor']\n",
    "    X_historical_enhanced['rainfall_intensity'] = X_historical_enhanced['max_daily_rainfall'] / (X_historical_enhanced['average_rainfall'] + 1e-6)\n",
    "    X_historical_enhanced['elevation_slope'] = X_historical_enhanced['elevation'] * X_historical_enhanced['slope']\n",
    "    X_historical_enhanced['composite_risk'] = (X_historical_enhanced['total_rainfall'] * X_historical_enhanced['traffic_factor']) / (X_historical_enhanced['drainage_density'] + 1e-6)\n",
    "    X_historical_enhanced['terrain_index'] = X_historical_enhanced['slope'] / (X_historical_enhanced['elevation'] + 1e-6)\n",
    "    X_historical_enhanced['rainfall_drainage_ratio'] = X_historical_enhanced['total_rainfall'] / (X_historical_enhanced['drainage_density'] + 1e-6)\n",
    "\n",
    "    X_historical_poly = poly.transform(X_historical_enhanced[poly_features])\n",
    "    for i, name in enumerate(poly_feature_names):\n",
    "        X_historical_enhanced[name] = X_historical_poly[:, i]\n",
    "    \n",
    "    historical_proba = best_binary_model.predict_proba(X_historical_enhanced)[:, 1]\n",
    "    historical_preds = (historical_proba >= best_final_threshold).astype(int)\n",
    "\n",
    "    print(\"\\n--- Historical Event Simulation Report ---\")\n",
    "    avg_prob_baseline = np.mean(proba_final)\n",
    "    avg_prob_event = np.mean(historical_proba)\n",
    "    \n",
    "    baseline_high_risk_count_binary = (proba_final >= best_final_threshold).sum()\n",
    "\n",
    "    print(f\"  Avg. Risk Probability (Baseline Test Set): {avg_prob_baseline:.2%}\")\n",
    "    print(f\"  Avg. Risk Probability (3-Day Event):      {avg_prob_event:.2%}\")\n",
    "    print(f\"  Wards Predicted 'High Risk' (Baseline): {baseline_high_risk_count_binary}\")\n",
    "    print(f\"  Wards Predicted 'High Risk' (3-Day Event): {historical_preds.sum()}\")\n",
    "    \n",
    "    if historical_preds.sum() > baseline_high_risk_count_binary:\n",
    "        print(f\"\\n\u2705 Model shows correct sensitivity: High-risk wards increased from {baseline_high_risk_count_binary} to {historical_preds.sum()}.\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f Model does not show an increase in the number of high-risk wards for this event.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeseries-cv-header",
   "metadata": {},
   "source": [
    "## 9. Robust Validation: Expanding Window Time-Series CV\n",
    "\n",
    "This final section implements the robust validation technique you described. Instead of a single 80/20 split, we will use an **Expanding Window Time-Series Cross-Validator**.\n",
    "\n",
    "This validator will split our **date columns** into 5 chronological folds. For each fold, we will re-run the *entire* feature engineering, model training, and threshold-tuning pipeline. This simulates 5 separate 'deployments' of the model at different points in time.\n",
    "\n",
    "The result will be a **Mean Accuracy** and **Standard Deviation**, which gives us a much more reliable and defensible measure of our model's true performance. \n",
    "\n",
    "****\n",
    "\n",
    "This process is computationally expensive as it trains the full model 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timeseries-cv-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udd2c Starting Robust Validation: 5-Fold Expanding Window Time-Series CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "fold_scores = []\n",
    "\n",
    "if len(date_columns) < n_splits * 2:\n",
    "    print(f\"Error: Not enough date columns ({len(date_columns)}) for {n_splits} splits.\")\n",
    "else:\n",
    "    fold_num = 1\n",
    "    for train_indices, test_indices in tscv.split(date_columns):\n",
    "        print(f\"\\n--- FOLD {fold_num}/{n_splits} --- \")\n",
    "        cv_train_dates = [date_columns[i] for i in train_indices]\n",
    "        cv_test_dates = [date_columns[i] for i in test_indices]\n",
    "        print(f\"  Training on {len(cv_train_dates)} dates (up to {cv_train_dates[-1]})\")\n",
    "        print(f\"  Testing on {len(cv_test_dates)} dates (from {cv_test_dates[0]} to {cv_test_dates[-1]})\")\n",
    "\n",
    "\n",
    "        cv_wards_train = copy.deepcopy(wards)\n",
    "        cv_wards_test = copy.deepcopy(wards)\n",
    "\n",
    "        cv_wards_train['average_rainfall'] = cv_wards_train[cv_train_dates].mean(axis=1)\n",
    "        cv_wards_train['max_daily_rainfall'] = cv_wards_train[cv_train_dates].max(axis=1)\n",
    "        cv_wards_train['rainfall_std'] = cv_wards_train[cv_train_dates].std(axis=1)\n",
    "        cv_wards_train['total_rainfall'] = cv_wards_train[cv_train_dates].sum(axis=1)\n",
    "\n",
    "        cv_wards_test['average_rainfall'] = cv_wards_test[cv_test_dates].mean(axis=1)\n",
    "        cv_wards_test['max_daily_rainfall'] = cv_wards_test[cv_test_dates].max(axis=1)\n",
    "        cv_wards_test['rainfall_std'] = cv_wards_test[cv_test_dates].std(axis=1)\n",
    "        cv_wards_test['total_rainfall'] = cv_wards_test[cv_test_dates].sum(axis=1)\n",
    "\n",
    "        cv_scaler_train = MinMaxScaler()\n",
    "        cv_scaler_features = ['elevation', 'slope', 'average_rainfall', 'drainage_density', 'traffic_factor']\n",
    "        cv_wards_train[['elevation_norm', 'slope_norm', 'rainfall_norm', 'drainage_norm', 'traffic_norm']] = cv_scaler_train.fit_transform(\n",
    "            cv_wards_train[cv_scaler_features]\n",
    "        )\n",
    "        cv_wards_train['hazard_score'] = (1 - cv_wards_train['elevation_norm']) + (1 - cv_wards_train['slope_norm']) + cv_wards_train['rainfall_norm'] + (1 - cv_wards_train['drainage_norm']) + cv_wards_train['traffic_norm']\n",
    "        cv_hazard_bins = pd.qcut(cv_wards_train['hazard_score'], q=3, retbins=True, duplicates='drop')[1]\n",
    "        cv_wards_train['hazard_class'] = pd.cut(cv_wards_train['hazard_score'], bins=cv_hazard_bins, labels=['Low Risk', 'Medium Risk', 'High Risk'], include_lowest=True)\n",
    "\n",
    "        cv_wards_test[['elevation_norm', 'slope_norm', 'rainfall_norm', 'drainage_norm', 'traffic_norm']] = cv_scaler_train.transform(\n",
    "            cv_wards_test[cv_scaler_features]\n",
    "        )\n",
    "        cv_wards_test['hazard_score'] = (1 - cv_wards_test['elevation_norm']) + (1 - cv_wards_test['slope_norm']) + cv_wards_test['rainfall_norm'] + (1 - cv_wards_test['drainage_norm']) + cv_wards_test['traffic_norm']\n",
    "        cv_wards_test['hazard_class'] = pd.cut(cv_wards_test['hazard_score'], bins=cv_hazard_bins, labels=['Low Risk', 'Medium Risk', 'High Risk'], include_lowest=True)\n",
    "\n",
    "        if cv_wards_test['hazard_class'].isna().any():\n",
    "            cv_wards_test['hazard_class'] = cv_wards_test['hazard_class'].astype(str).replace('nan', 'Medium Risk')\n",
    "        if cv_wards_train['hazard_class'].isna().any():\n",
    "            cv_wards_train['hazard_class'] = cv_wards_train['hazard_class'].astype(str).replace('nan', 'Medium Risk')\n",
    "\n",
    "        cv_wards_train['rainfall_cv'] = cv_wards_train['rainfall_std'] / (cv_wards_train['average_rainfall'] + 1e-6)\n",
    "        cv_wards_train['rainfall_skew'] = cv_wards_train[cv_train_dates].skew(axis=1)\n",
    "        cv_wards_train['rainfall_kurtosis'] = cv_wards_train[cv_train_dates].kurtosis(axis=1)\n",
    "        cv_wards_train['heavy_rain_days'] = (cv_wards_train[cv_train_dates] > cv_wards_train[cv_train_dates].quantile(0.95, axis=1).mean()).sum(axis=1)\n",
    "        cv_wards_train['dry_days'] = (cv_wards_train[cv_train_dates] < cv_wards_train[cv_train_dates].quantile(0.05, axis=1).mean()).sum(axis=1)\n",
    "        cv_wards_train['rainfall_iqr'] = cv_wards_train[cv_train_dates].quantile(0.75, axis=1) - cv_wards_train[cv_train_dates].quantile(0.25, axis=1)\n",
    "\n",
    "        cv_wards_test['rainfall_cv'] = cv_wards_test['rainfall_std'] / (cv_wards_test['average_rainfall'] + 1e-6)\n",
    "        cv_wards_test['rainfall_skew'] = cv_wards_test[cv_test_dates].skew(axis=1)\n",
    "        cv_wards_test['rainfall_kurtosis'] = cv_wards_test[cv_test_dates].kurtosis(axis=1)\n",
    "        cv_wards_test['heavy_rain_days'] = (cv_wards_test[cv_test_dates] > cv_wards_test[cv_test_dates].quantile(0.95, axis=1).mean()).sum(axis=1)\n",
    "        cv_wards_test['dry_days'] = (cv_wards_test[cv_test_dates] < cv_wards_test[cv_test_dates].quantile(0.05, axis=1).mean()).sum(axis=1)\n",
    "        cv_wards_test['rainfall_iqr'] = cv_wards_test[cv_test_dates].quantile(0.75, axis=1) - cv_wards_test[cv_test_dates].quantile(0.25, axis=1)\n",
    "\n",
    "        cv_features = ['elevation', 'slope', 'average_rainfall', 'max_daily_rainfall', 'rainfall_std', 'total_rainfall', 'land_cover', 'drainage_density', 'traffic_factor', 'rainfall_cv', 'rainfall_skew', 'rainfall_kurtosis', 'heavy_rain_days', 'dry_days', 'rainfall_iqr']\n",
    "        cv_numeric_features = [f for f in cv_features if f != 'land_cover']\n",
    "\n",
    "        cv_wards_train[cv_numeric_features] = cv_wards_train[cv_numeric_features].apply(pd.to_numeric, errors='coerce').fillna(cv_wards_train[cv_numeric_features].median())\n",
    "        cv_wards_test[cv_numeric_features] = cv_wards_test[cv_numeric_features].apply(pd.to_numeric, errors='coerce').fillna(cv_wards_test[cv_numeric_features].median())\n",
    "        cv_wards_train['land_cover'].fillna(cv_wards_train['land_cover'].mode().iloc[0], inplace=True)\n",
    "        cv_wards_test['land_cover'].fillna(cv_wards_test['land_cover'].mode().iloc[0], inplace=True)\n",
    "        \n",
    "        cv_X_train = cv_wards_train[cv_features]\n",
    "        cv_y_train = cv_wards_train['hazard_class']\n",
    "        cv_X_test = cv_wards_test[cv_features]\n",
    "        cv_y_test = cv_wards_test['hazard_class']\n",
    "\n",
    "        cv_X_train_enhanced = cv_X_train.copy()\n",
    "        cv_X_test_enhanced = cv_X_test.copy()\n",
    "\n",
    "        for df in [cv_X_train_enhanced, cv_X_test_enhanced]:\n",
    "            df['elevation_rainfall'] = df['elevation'] * df['average_rainfall']\n",
    "            df['slope_rainfall'] = df['slope'] * df['average_rainfall']\n",
    "            df['drainage_traffic'] = df['drainage_density'] * df['traffic_factor']\n",
    "            df['rainfall_intensity'] = df['max_daily_rainfall'] / (df['average_rainfall'] + 1e-6)\n",
    "            df['elevation_slope'] = df['elevation'] * df['slope']\n",
    "            df['composite_risk'] = (df['total_rainfall'] * df['traffic_factor']) / (df['drainage_density'] + 1e-6)\n",
    "            df['terrain_index'] = df['slope'] / (df['elevation'] + 1e-6)\n",
    "            df['rainfall_drainage_ratio'] = df['total_rainfall'] / (df['drainage_density'] + 1e-6)\n",
    "\n",
    "        cv_poly_features = ['average_rainfall', 'slope', 'drainage_density']\n",
    "        cv_poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "        cv_X_train_poly = cv_poly.fit_transform(cv_X_train_enhanced[cv_poly_features])\n",
    "        cv_X_test_poly = cv_poly.transform(cv_X_test_enhanced[cv_poly_features])\n",
    "\n",
    "        cv_poly_feature_names = [f'poly_{i}' for i in range(cv_X_train_poly.shape[1])]\n",
    "        for i, name in enumerate(cv_poly_feature_names):\n",
    "            cv_X_train_enhanced[name] = cv_X_train_poly[:, i]\n",
    "            cv_X_test_enhanced[name] = cv_X_test_poly[:, i]\n",
    "\n",
    "        cv_numeric_features_enhanced = [col for col in cv_X_train_enhanced.columns if col != 'land_cover']\n",
    "        cv_X_train = cv_X_train_enhanced\n",
    "        cv_X_test = cv_X_test_enhanced\n",
    "\n",
    "        cv_y_train_binary = (cv_y_train == 'High Risk').astype(int)\n",
    "        cv_y_test_binary = (cv_y_test == 'High Risk').astype(int)\n",
    "\n",
    "        cv_categorical_features = ['land_cover']\n",
    "        cv_preprocessor_binary = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), cv_numeric_features_enhanced),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cv_categorical_features)\n",
    "            ])\n",
    "\n",
    "        cv_smote = SMOTE(sampling_strategy=0.85, k_neighbors=7, random_state=42)\n",
    "        if len(np.unique(cv_y_train_binary)) < 2:\n",
    "            print(\"  Skipping fold: Not enough classes in training data.\")\n",
    "            fold_num += 1\n",
    "            continue\n",
    "            \n",
    "        cv_X_train_smote, cv_y_train_smote = cv_smote.fit_resample(cv_X_train, cv_y_train_binary)\n",
    "        \n",
    "        cv_xgb_tuned = xgb.XGBClassifier(n_estimators=500, learning_rate=0.02, max_depth=8, subsample=0.85, colsample_bytree=0.8, reg_alpha=0.15, reg_lambda=1.2, gamma=0.1, min_child_weight=2, random_state=42, objective='binary:logistic', eval_metric='logloss')\n",
    "        cv_lgb_tuned = lgb.LGBMClassifier(n_estimators=500, learning_rate=0.02, max_depth=9, num_leaves=40, subsample=0.85, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0, random_state=42, verbose=-1)\n",
    "\n",
    "        cv_pipe_xgb = Pipeline([('prep', cv_preprocessor_binary), ('clf', cv_xgb_tuned)])\n",
    "        cv_pipe_lgb = Pipeline([('prep', cv_preprocessor_binary), ('clf', cv_lgb_tuned)])\n",
    "\n",
    "        cv_splitter_meta = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "        (cv_train_idx, cv_val_idx) = next(cv_splitter_meta.split(cv_X_train_smote, cv_y_train_smote))\n",
    "\n",
    "        cv_X_tr_base, cv_X_val_meta = cv_X_train_smote.iloc[cv_train_idx], cv_X_train_smote.iloc[cv_val_idx]\n",
    "        cv_y_tr_base, cv_y_val_meta = cv_y_train_smote.iloc[cv_train_idx], cv_y_train_smote.iloc[cv_val_idx]\n",
    "\n",
    "        cv_pipe_xgb.fit(cv_X_tr_base, cv_y_tr_base)\n",
    "        cv_pipe_lgb.fit(cv_X_tr_base, cv_y_tr_base)\n",
    "\n",
    "        cv_xgb_val_proba = cv_pipe_xgb.predict_proba(cv_X_val_meta)[:, 1]\n",
    "        cv_lgb_val_proba = cv_pipe_lgb.predict_proba(cv_X_val_meta)[:, 1]\n",
    "        cv_meta_X_val = np.column_stack([cv_xgb_val_proba, cv_lgb_val_proba])\n",
    "\n",
    "        cv_meta_clf = LogisticRegression(C=0.1, solver='liblinear', penalty='l1', class_weight='balanced', random_state=42)\n",
    "        cv_meta_clf.fit(cv_meta_X_val, cv_y_val_meta)\n",
    "\n",
    "        cv_pipe_xgb.fit(cv_X_train_smote, cv_y_train_smote)\n",
    "        cv_pipe_lgb.fit(cv_X_train_smote, cv_y_train_smote)\n",
    "\n",
    "        cv_xgb_proba_test = cv_pipe_xgb.predict_proba(cv_X_test)[:, 1]\n",
    "        cv_lgb_proba_test = cv_pipe_lgb.predict_proba(cv_X_test)[:, 1]\n",
    "        cv_meta_X_test = np.column_stack([cv_xgb_proba_test, cv_lgb_proba_test])\n",
    "        cv_meta_proba = cv_meta_clf.predict_proba(cv_meta_X_test)[:, 1]\n",
    "\n",
    "        cv_best_acc = 0\n",
    "        for thr in np.arange(0.2, 0.8, 0.005):\n",
    "            cv_preds = (cv_meta_proba >= thr).astype(int)\n",
    "            acc = accuracy_score(cv_y_test_binary, cv_preds)\n",
    "            if acc > cv_best_acc:\n",
    "                cv_best_acc = acc\n",
    "        \n",
    "        print(f\"  Fold {fold_num} Best Accuracy: {cv_best_acc:.4f}\")\n",
    "        fold_scores.append(cv_best_acc)\n",
    "        fold_num += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 Robust Validation Complete: Final Report\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if fold_scores:\n",
    "    mean_acc = np.mean(fold_scores)\n",
    "    std_acc = np.std(fold_scores)\n",
    "    \n",
    "    print(f\"Scores per fold: {[f'{s*100:.2f}%' for s in fold_scores]}\")\n",
    "    print(\"\\n--- Summary --- \")\n",
    "    print(f\"Mean Accuracy:   {mean_acc:.4f}  ({mean_acc*100:.2f}%) \u2705\")\n",
    "    print(f\"Std. Deviation:  {std_acc:.4f}  (\u00b1 {std_acc*100:.2f}%) \")\n",
    "    \n",
    "    print(f\"\\nThis result is a highly reliable estimate of the model's performance.\")\n",
    "else:\n",
    "    print(\"Cross-validation did not produce any scores. Please check data and split logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15764d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import nbformat\n",
    "import base64\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "from shutil import copy2\n",
    "\n",
    "NB_PATH = r\"D:\\Notes\\Rpb2\\Untitled-1.ipynb\"  # update if your notebook is at a different path\n",
    "OUT_DIR = Path(NB_PATH).parent / \"extracted_images\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_slug_counters = {}\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\-_. ]+\", \"\", s)\n",
    "    s = re.sub(r\"[\\s]+\", \"-\", s)\n",
    "    if not s:\n",
    "        s = \"image\"\n",
    "    count = _slug_counters.get(s, 0)\n",
    "    _slug_counters[s] = count + 1\n",
    "    if count:\n",
    "        return f\"{s}-{count}\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def find_prev_heading(nb, idx):\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        cell = nb.cells[j]\n",
    "        if cell.get(\"cell_type\") == \"markdown\":\n",
    "            src = cell.get(\"source\", \"\")\n",
    "            for line in src.splitlines():\n",
    "                m = re.match(r\"^#{1,6}\\s+(.*)\", line)\n",
    "                if m:\n",
    "                    return m.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_bytes(bts: bytes, title: str, ext: str) -> Path:\n",
    "    slug = slugify(title)\n",
    "    if not ext.startswith('.'):\n",
    "        ext = f\".{ext}\"\n",
    "    fname = f\"{slug}{ext}\"\n",
    "    path = OUT_DIR / fname\n",
    "    i = 1\n",
    "    while path.exists():\n",
    "        path = OUT_DIR / f\"{slug}-{i}{ext}\"\n",
    "        i += 1\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(bts)\n",
    "    return path\n",
    "\n",
    "\n",
    "nb = nbformat.read(NB_PATH, as_version=4)\n",
    "saved = []\n",
    "\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    ctype = cell.get(\"cell_type\")\n",
    "\n",
    "    if ctype == \"markdown\":\n",
    "        md = cell.get(\"source\", \"\")\n",
    "        for alt, src in re.findall(r\"!\\[([^\\]]*)\\]\\(([^\\)]+)\\)\", md):\n",
    "            title = alt.strip() or find_prev_heading(nb, i) or f\"markdown-image-{i}\"\n",
    "            src = src.strip()\n",
    "            if src.startswith(\"data:\"):\n",
    "                m = re.match(r\"data:(image/[a-zA-Z0-9.+-]+);base64,(.*)$\", src, flags=re.S)\n",
    "                if m:\n",
    "                    mediatype, b64 = m.groups()\n",
    "                    ext = mimetypes.guess_extension(mediatype) or \".png\"\n",
    "                    bts = base64.b64decode(b64)\n",
    "                    p = save_bytes(bts, title, ext)\n",
    "                    saved.append((str(p), title, mediatype))\n",
    "                else:\n",
    "                    print(f\"Skipped unknown data URI at markdown image {i}\")\n",
    "            elif src.startswith(\"http://\") or src.startswith(\"https://\"):\n",
    "                print(f\"Found remote image URL (not downloaded): {src} (title: {title})\")\n",
    "            else:\n",
    "                src_path = (Path(NB_PATH).parent / src).resolve()\n",
    "                if src_path.exists():\n",
    "                    ext = src_path.suffix or \".png\"\n",
    "                    dest = OUT_DIR / f\"{slugify(title)}{ext}\"\n",
    "                    copy2(src_path, dest)\n",
    "                    saved.append((str(dest), title, f\"file:{src_path}\"))\n",
    "                else:\n",
    "                    print(f\"Referenced image file not found: {src_path} (from markdown)\")\n",
    "\n",
    "    if ctype == \"code\":\n",
    "        outputs = cell.get(\"outputs\", []) or []\n",
    "        for out in outputs:\n",
    "            data = out.get(\"data\") or {}\n",
    "            for mime in (\"image/png\", \"image/jpeg\", \"image/jpg\", \"image/svg+xml\"):\n",
    "                if mime in data:\n",
    "                    payload = data[mime]\n",
    "                    if isinstance(payload, list):\n",
    "                        payload = \"\".join(payload)\n",
    "                    if mime == \"image/svg+xml\":\n",
    "                        bts = payload.encode(\"utf-8\") if isinstance(payload, str) else payload\n",
    "                        ext = \".svg\"\n",
    "                    else:\n",
    "                        if isinstance(payload, str):\n",
    "                            try:\n",
    "                                bts = base64.b64decode(payload)\n",
    "                            except Exception:\n",
    "                                bts = payload\n",
    "                        else:\n",
    "                            bts = payload\n",
    "                        ext = \".png\" if mime == \"image/png\" else \".jpg\"\n",
    "\n",
    "                    title = find_prev_heading(nb, i) or f\"output-image-{i}\"\n",
    "                    p = save_bytes(bts, title, ext)\n",
    "                    saved.append((str(p), title, mime))\n",
    "\n",
    "print(\"\\nExtraction complete. Saved images:\")\n",
    "for path, title, kind in saved:\n",
    "    print(f\" - {path}  (title: {title}, kind: {kind})\")\n",
    "\n",
    "if not saved:\n",
    "    print(\"No images were found or extracted. Check notebook path and cell contents.\")\n",
    "\n",
    "saved\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}